J48=-U\nUse unpruned tree.\n\n-C <pruning confidence>\nSet confidence threshold for pruning. (default 0.25)\n\n-M <minimum number of instances>\nSet minimum number of instances per leaf. (default 2)\n\n-R\nUse reduced error pruning.\n\n-N <number of folds>\nSet number of folds for reduced error pruning. One fold is used as pruning set. (default 3)\n\n-B\nUse binary splits only.\n\n-S\nDon't perform subtree raising.\n\n-L\nDo not clean up after the tree has been built.\n\n-A\nLaplace smoothing for predicted probabilities.\n\n-Q <seed>\nSeed for random data shuffling (default 1).
NaiveBayes=-K\nUse kernel density estimator rather than normal distribution for numeric attributes\n\n-D\nUse supervised discretization to process numeric attributes\n\n-O\nDisplay model in old format (good when there are many classes)
SMO=-D\nIf set, classifier is run in debug mode and may output additional info to the console\n\n-no-checks\nTurns off all checks - use with caution! Turning them off assumes that data is purely numeric, doesn't contain any missing values, and has a nominal class. Turning them off also means that no header information will be stored if the machine is linear. Finally, it also assumes that no instance has a weight equal to 0. (default: checks on)\n\n-C <double>\nThe complexity constant C. (default 1)\n\n-N\nWhether to 0=normalize/1=standardize/2=neither. (default 0=normalize)\n\n-L <double>\nThe tolerance parameter. (default 1.0e-3)\n\n-P <double>\nThe epsilon for round-off error. (default 1.0e-12)\n\n-M\nFit logistic models to SVM outputs.\n\n-V <double>\nThe number of folds for the internal cross-validation. (default -1, use training data)\n\n-W <double>\nThe random number seed. (default 1)\n\n-K <classname and parameters>\nThe Kernel to use. (default: weka.classifiers.functions.supportVector.PolyKernel)
IBk=-I\nWeight neighbours by the inverse of their distance (use when k > 1)\n\n-F\nWeight neighbours by 1 - their distance (use when k > 1)\n\n-K <number of neighbors>\nNumber of nearest neighbours (k) used in classification. (Default = 1)\n\n-E\nMinimise mean squared error rather than mean absolute error when using -X option with numeric prediction.\n\n-W <window size>\nMaximum number of training instances maintained. Training instances are dropped FIFO. (Default = no window)\n\n-X\nSelect the number of nearest neighbours between 1 and the k value specified using hold-one-out evaluation on the training data (use when k > 1)\n\n-A\nThe nearest neighbour search algorithm to use (default: weka.core.neighboursearch.LinearNNSearch).
KStar=-B <num>\nManual blend setting (default 20%)\n\n-E\nEnable entropic auto-blend setting (symbolic class only)\n\n-M <char>\nSpecify the missing value treatment mode (default a) Valid options are: a(verage), d(elete), m(axdiff), n(ormal)
PART=-C <pruning confidence>\nSet confidence threshold for pruning. (default 0.25)\n\n-M <minimum number of objects>\nSet minimum number of objects per leaf. (default 2)\n\n-R\nUse reduced error pruning.\n\n-N <number of folds>\nSet number of folds for reduced error pruning. One fold is used as pruning set. (default 3)\n\n-B\nUse binary splits only.\n\n-U\nGenerate unpruned decision list.\n\n-J\nDo not use MDL correction for info gain on numeric attributes.\n\n-Q <seed>\nSeed for random data shuffling (default 1).\n\n-doNotMakeSplitPointActualValue\nDo not make split point actual value.
JRip=-F <number of folds>\nSet number of folds for REP One fold is used as pruning set. (default 3)\n\n-N <min. weights>\nSet the minimal weights of instances within a split. (default 2.0)\n\n-O <number of runs>\nSet the number of runs of optimizations. (Default: 2)\n\n-D\nSet whether turn on the debug mode (Default: false)\n\n-S <seed>\nThe seed of randomization\n(Default: 1)\n\n-E\nWhether NOT check the error rate>=0.5 in stopping criteria  (default: check)\n\n-P\nWhether NOT use pruning (default: use pruning)
LogitBoost=-Q\nUse resampling instead of reweighting for boosting.\n\n-P <percent>\nPercentage of weight mass to base training on. (default 100, reduce to around 90 speed up)\n\n-L <num>\nThreshold on the improvement of the likelihood. (default -Double.MAX_VALUE)\n\n-H <num>\nShrinkage parameter. (default 1)\n\n-Z <num>\nZ max threshold for responses. (default 3)\n\n-O <int>\nThe size of the thread pool, for example, the number of cores in the CPU. (default 1)\n\n-E <int>\nThe number of threads to use for batch prediction, which should be >= size of thread pool. (default 1)\n\n-S <num>\nRandom number seed. (default 1)\n\n-I <num>\nNumber of iterations. (default 10)\n\n-W\nFull name of base classifier. (default: weka.classifiers.trees.DecisionStump)\n\n-output-debug-info \nIf set, classifier is run in debug mode and may output additional info to the console\n\n-do-not-check-capabilities\nIf set, classifier capabilities are not checked before classifier is built (use with caution)
LMT=-B\nBinary splits (convert nominal attributes to binary ones)\n\n-R\nSplit on residuals instead of class values\n\n-C\nUse cross-validation for boosting at all nodes (i.e., disable heuristic)\n\n-P\nUse error on probabilities instead of misclassification error for stopping criterion of LogitBoost.\n\n-I <numIterations>\nSet fixed number of iterations for LogitBoost (instead of using cross-validation)\n\n-M <numInstances>\nSet minimum number of instances at which a node can be split (default 15)\n\n-W <beta>\nSet beta for weight trimming for LogitBoost. Set to 0 (default) for no weight trimming.\n\n-A\nThe AIC is used to choose the best iteration.\n\n-doNotMakeSplitPointActualValue\nDo not make split point actual value.
NBMU=-D\nIf set, classifier is run in debug mode and may output additional info to the console
REPTree=-M <minimum number of instances>\nSet minimum number of instances per leaf (default 2).\n\n-V <minimum variance for split>\nSet minimum numeric class variance proportion of train variance for split (default 1e-3).\n\n-N <number of folds>\nNumber of folds for reduced error pruning (default 3).\n\n-S <seed>\nSeed for random data shuffling (default 1).\n\n-P\nNo pruning.\n\n-L\nMaximum tree depth (default -1, no maximum)
DecisionTable=-S <search method specification>\nFull class name of search method, followed by its options. eg: "weka.attributeSelection.BestFirst -D 1" (default weka.attributeSelection.BestFirst)\n\n-X <number of folds>\nUse cross validation to evaluate features. Use number of folds = 1 for leave one out CV. (Default = leave one out CV)\n\n-E <acc | rmse | mae | auc>\nPerformance evaluation measure to use for selecting attributes. (Default = accuracy for discrete class and rmse for numeric class)\n\n-I\nUse nearest neighbour instead of global table majority.\n\n-R\nDisplay decision table rules.\n\nOptions specific to search method weka.attributeSelection.BestFirst:\n\n-P <start set>\nSpecify a starting set of attributes. Eg. 1,3,5-7.\n\n-D <0 = backward | 1 = forward | 2 = bi-directional>\nDirection of search. (default = 1).\n\n-N <num>\nNumber of non-improving nodes to consider before terminating search.\n\n-S <num>\nSize of lookup cache for evaluated subsets. Expressed as a multiple of the number of attributes in the data set. (default = 1)
BayesNet=-D\nDo not use ADTree data structure\n\n-B <BIF file>\nBIF file to compare with\n\n-Q weka.classifiers.bayes.net.search.SearchAlgorithm\nSearch algorithm\n\n-E weka.classifiers.bayes.net.estimate.SimpleEstimator\nEstimator algorithm